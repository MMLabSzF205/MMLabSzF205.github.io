<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="Chinese, English">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.4.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="F205学习小组博客">
<meta property="og:url" content="http://MMLabSzF205.github.io/index.html">
<meta property="og:site_name" content="F205学习小组博客">
<meta property="og:locale" content="Chinese, English">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="F205学习小组博客">






  <link rel="canonical" href="http://MMLabSzF205.github.io/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>F205学习小组博客</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="Chinese, English">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">F205学习小组博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://MMLabSzF205.github.io/2019/图卷积神经网络入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MMLabSzF205">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="F205学习小组博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/图卷积神经网络入门/" itemprop="url">
                  Untitled
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-24 17:23:44 / Modified: 17:27:39" itemprop="dateCreated datePublished" datetime="2019-06-24T17:23:44+08:00">2019-06-24</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>title: 图卷积神经网络入门<br>date: 2019-06-24</p>
<h2 id="tags"><a href="#tags" class="headerlink" title="tags:"></a>tags:</h2><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1.1. 卷积"></a>1.1. 卷积</h2><p><strong>1.局部感受野</strong></p>
<p><strong>2.参数共享</strong><br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=1499f41d314a700d2416606bc606ee33" alt="1499f41d314a700d2416606bc606ee33.gif"><br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=3b50d70252d75c39821e26a0c6bc9bc7" alt="3b50d70252d75c39821e26a0c6bc9bc7.png"></p>
<h2 id="1-2-为什么需要图卷积"><a href="#1-2-为什么需要图卷积" class="headerlink" title="1.2. 为什么需要图卷积"></a>1.2. 为什么需要图卷积</h2><p><img src="/2019/图卷积神经网络入门/4DFE5464-E8E2-4A20-BD8D-F8F26DFCAE31.png" alt="517f816afc86536f35a4ba29509aeb50"><br><strong>为什么不能直接使用卷积？</strong></p>
<ul>
<li>每个节点的邻居数目不一定相同</li>
</ul>
<p><strong>那么问题来了，如何定义不规则图结构上的卷积呢？</strong><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=d437ddd8d735e3d6dd2e4ef2b6058953" alt="d437ddd8d735e3d6dd2e4ef2b6058953.jpeg"></p>
<h2 id="1-3-图卷积直观理解"><a href="#1-3-图卷积直观理解" class="headerlink" title="1.3. 图卷积直观理解"></a>1.3. 图卷积直观理解</h2><p>定义一：</p>
<script type="math/tex; mode=display">
f\left(H^{(l)}, A\right)=\sigma\left(A H^{(l)} W^{(l)}\right)</script><p>A是邻接矩阵<br>定义二:</p>
<script type="math/tex; mode=display">
f\left(H^{(l)}, A\right)=\sigma\left(\hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)</script><ol>
<li><strong>传递</strong>信息给相邻节点<br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=ca83187d8628ff8f31c7dc663516dafd" alt="ca83187d8628ff8f31c7dc663516dafd.gif"></li>
<li><p><strong>接受</strong>来自相邻节点的信息<br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=b4acb10553c74678ddab54ea285faa0d" alt="b4acb10553c74678ddab54ea285faa0d.gif"></p>
</li>
<li><p>计算新的表示<br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=b8d5806ef71f450298abf2552c5b7b69" alt="b8d5806ef71f450298abf2552c5b7b69.gif"><br><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=833da1cab7124891bd32d0f39902dedf" alt="833da1cab7124891bd32d0f39902dedf.gif"></p>
</li>
</ol>
<p><strong>VS. cnn卷积</strong><br>cnn卷积也可以算是接受来自附近节点的信息，然后接受并更新，随着层数增加，感受野逐渐增加</p>
<h2 id="1-4-图卷积应用"><a href="#1-4-图卷积应用" class="headerlink" title="1.4. 图卷积应用"></a>1.4. 图卷积应用</h2><p><img src="/2019/图卷积神经网络入门/1A956D8D-4FEF-4878-B09A-11E470ADC4B0.png" alt="f169b88eed3d00da5b9bf30edc4f0a9a"><br>图片和文本经过一定处理后也是一个不规则图结构，cnn卷积，lstm,transformer等网络无法处理</p>
<h1 id="2-图上的卷积定义"><a href="#2-图上的卷积定义" class="headerlink" title="2. 图上的卷积定义"></a>2. 图上的卷积定义</h1><h2 id="2-1-基础"><a href="#2-1-基础" class="headerlink" title="2.1. 基础"></a>2.1. 基础</h2><ol>
<li>傅里叶变换及其逆变换<script type="math/tex; mode=display">
\mathcal{F}\{f\}(v)=\int_{\mathbb{R}} f(x) e^{-2 \pi i x \cdot v} d x</script><script type="math/tex; mode=display">
\mathcal{F}^{-1}\{f\}(x)=\int_{\mathbb{R}} F(v) e^{2 \pi i x \cdot v} d v</script></li>
</ol>
<p>我们把函数看成是一个向量，上面的$e^{2 \pi i x v}(v\in R)$可以看成是一组正交基,那么傅里叶变换做的事就是求出原向量在各个正交基上的投影，傅里叶逆变换自然就是各个正交基加权求和得到原向量</p>
<ol>
<li><p>卷积定理（时域上的卷积，频域上的乘积）</p>
<script type="math/tex; mode=display">
f * g=\mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\}\}</script></li>
<li><p>Laplacian算子</p>
</li>
</ol>
<script type="math/tex; mode=display">
D(i, j)=\left\{\begin{array}{ll}{d_{i}} & {\text { if } i=j} \\ {0} & {\text { otherwise }}\end{array}\right.</script><script type="math/tex; mode=display">
A(i, j)=\left\{\begin{array}{ll}{1} & {\text { if } x_{i} \sim x_{j}} \\ {0} & {\text { otherwise }}\end{array}\right.</script><script type="math/tex; mode=display">
L=D-A</script><p>标准化的Laplacian算子</p>
<script type="math/tex; mode=display">
L=I_{N}-D^{-\frac{1}{2}} A D^{-\frac{1}{2}}</script><p>对角化，U为正交矩阵</p>
<script type="math/tex; mode=display">
L=U \Lambda U^{T}</script><ol>
<li>切比雪夫多项式<script type="math/tex; mode=display">
T_{0}(x)=1, T_{1}(x)=x, T_{n+1}(x)=2 x T_{n}(x)-T_{n-1}(x)</script>切比雪夫多项式在逼近理论中有重要的应用</li>
</ol>
<h2 id="2-2-图卷积定义"><a href="#2-2-图卷积定义" class="headerlink" title="2.2. 图卷积定义"></a>2.2. 图卷积定义</h2><p><img src="evernotecid://218D68B6-39DF-4241-861D-CFBC06CA44DD/appyinxiangcom/22359036/ENNote/p38?hash=4fb7f437d95a5a93d655dfe317781283" alt="4fb7f437d95a5a93d655dfe317781283.jpeg"><br>$f * g$在图上的卷积不好直接定义，那么我们能不能先定义图上的傅里叶变换，根据卷积定理，频域乘积再做逆变换得到卷积。这个图上的傅里叶变换是什么呢？<br>我们知道傅里叶变换的本质就是把原向量投影到以一组正交向量为基的空间中，在线性数据下，这组正交基是$e^{2 \pi i x v}(v\in R)$，那么在图结构下，这组正交基是什么呢？<br>前面提到的Laplacian算法对角化后得到的U就是我们要的图结构下的正交基。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>传统傅里叶变换</th>
<th>图傅里叶变换</th>
</tr>
</thead>
<tbody>
<tr>
<td>正交基</td>
<td>$e^{-2 \pi i x v}(v\in R)$</td>
<td>$U^T$</td>
</tr>
<tr>
<td>逆变换基</td>
<td>$e^{2 \pi i x v}(v\in R)$</td>
<td>$U$</td>
</tr>
<tr>
<td>维度</td>
<td>$\infty$</td>
<td>节点个数</td>
</tr>
</tbody>
</table>
</div>
<p>为什么是Laplacian算子的特征向量，而不是其他的矩阵？<br>Laplacian算子相当于定义了图上的二阶微分运算<br>一阶导数：</p>
<script type="math/tex; mode=display">
f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}</script><p>Laplacian算子就是二阶导数</p>
<script type="math/tex; mode=display">
\Delta f(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-2 f(x)+f(x-h)}{h^{2}}</script><p>图上的一阶导数定义</p>
<script type="math/tex; mode=display">
f_{* g}^{\prime}(x)=f(x)-f(y)</script><p>图上的Laplacian算子定义</p>
<script type="math/tex; mode=display">
\Delta_{* g} f^{\prime}(x)=\Sigma_{y \sim x} f(x)-f(y)</script><p>至于二阶微分运算和傅里叶变换之间的联系等着各位大佬补充Orz</p>
<p>图傅里叶变换</p>
<script type="math/tex; mode=display">
\mathcal{G F}\{x\}=U^{T} x</script><p>图逆傅里叶变换</p>
<script type="math/tex; mode=display">
\mathcal{I} \mathcal{G F}\{\hat x\}=U \hat x</script><p>现在我们有了图上的傅里叶变换，那么图上的卷积定理应该是怎样的呢？</p>
<script type="math/tex; mode=display">
g * x=U\left(U^{T} g \odot U^{T} x\right)</script><p>$\odot$是Hadamard乘积，两个列向量的Hadamard乘积可以看成是前一个列向量拉成对角阵再和第二个列向量做普通矩阵乘法<br>可以把$U^Tg$看成是特征值的函数，则有：</p>
<script type="math/tex; mode=display">g*x=Ug_{\theta}(\Lambda)U^T x</script><p>由此我们完成了图卷积定义<br>接下来就是用近似方法来降低参数量和计算复杂度<br><strong>设法避免对角化</strong></p>
<script type="math/tex; mode=display">
g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})</script><script type="math/tex; mode=display">
U \Lambda^{k} U^{T}=\left(U \Lambda U^{T}\right)^{k}=L^{k}</script><script type="math/tex; mode=display">
Ug_{\theta^{\prime}}(\Lambda)U^T = \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}({L})</script><p>设定$K=1$,$\mathbf L\approx{L}-\mathbf{I}_{N}$,卷积公式可以简化为<br>为何这样简化?点击<a href="https://arxiv.org/pdf/1812.08434.pdf" target="_blank" rel="noopener">here</a></p>
<script type="math/tex; mode=display">
\begin{aligned} g_{\theta^{\prime}} * x & \approx \theta\left(I_{N}+L\right) x \\ &=\theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x \end{aligned}</script><script type="math/tex; mode=display">
\begin{array}{l}{\  \tilde{A}=A+I_{N}, \quad \tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}} \\ {g_{\theta^{\prime}} * x=\theta\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}\right) x}\end{array}</script><script type="math/tex; mode=display">
H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)</script>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://MMLabSzF205.github.io/2019/图像问答（VQA）/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MMLabSzF205">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="F205学习小组博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/图像问答（VQA）/" itemprop="url">
                  VQA
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-23 14:00:00 / Modified: 13:56:40" itemprop="dateCreated datePublished" datetime="2019-06-23T14:00:00+08:00">2019-06-23</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VQA"><a href="#VQA" class="headerlink" title="VQA"></a>VQA</h1><h3 id="一、问题描述："><a href="#一、问题描述：" class="headerlink" title="一、问题描述："></a>一、问题描述：</h3><p>给定一张图片和一个与图片相关的问题，要求生成问题的答案。</p>
<h3 id="二、数据集"><a href="#二、数据集" class="headerlink" title="二、数据集"></a>二、数据集</h3><p>VQA 1.0<br>VQA 2.0<br>Visual Genome<br>Visual7W<br>COCO-QA</p>
<h3 id="三、评价指标"><a href="#三、评价指标" class="headerlink" title="三、评价指标"></a>三、评价指标</h3><p>通常每个image-question pair都由若干个不同的注释员给出自己的回答，以VQA数据集为例，每个pair都有10个人工给出的答案。VQA通常视作一个分类问题或文本生成问题，相应的评价指标有所不同：</p>
<h5 id="1-分类问题："><a href="#1-分类问题：" class="headerlink" title="1.分类问题："></a>1.分类问题：</h5><p>模型要在n种可能的回答中选择一个。常用的评判标准是准确率accuracy：<br><img src="/2019/图像问答（VQA）/A42963F8-67B1-4F97-AFCE-976402E400EC.png" alt="06954037c9fc8e87ff14f6c5daef57a2"></p>
<p>其中，a是模型给出的答案，Count(a)表示答案a在一个pair的所有人工答案中出现的次数。如果Count(a)超过3，就认为答案a是完全正确的。</p>
<h5 id="2-文本生成问题"><a href="#2-文本生成问题" class="headerlink" title="2.文本生成问题"></a>2.文本生成问题</h5><p>模型要提取出图像和问题的特征，把特征输入一个语言生成模型（比如RNN），输出一个任意长度的句子作为答案。（to-do）</p>
<h3 id="四、方法概述"><a href="#四、方法概述" class="headerlink" title="四、方法概述"></a>四、方法概述</h3><p>VQA问题要求模型能够理解图像和问题（文本）的语义，将这两种语义恰当地结合起来，然后预测答案。因此，VQA模型通常采用的结构如下图所示：<br><img src="/2019/图像问答（VQA）/67418B7A-CB71-4527-8818-739A2BAA7756.png" alt="675b71be55884ea532b6eeba8c06a3f4"></p>
<p>模型可以分为以下三个部分：</p>
<h4 id="1-从图像和问题中提取特征"><a href="#1-从图像和问题中提取特征" class="headerlink" title="1.从图像和问题中提取特征"></a>1.从图像和问题中提取特征</h4><p>（1）图像特征：低级（区域）特征（top-down，bottom-up），高级（语义）特征<br>（2）文本特征：RNN,BRNN,多层次（单词/短语/句子）特征<br>（3）注意力模型</p>
<h4 id="2-多模态特征融合"><a href="#2-多模态特征融合" class="headerlink" title="2.多模态特征融合"></a>2.多模态特征融合</h4><p>单线性：连接、相加、点乘<br>双线性：MCB, MLB, MFB, MUTAN</p>
<h4 id="3-预测答案"><a href="#3-预测答案" class="headerlink" title="3.预测答案"></a>3.预测答案</h4><p>单分类：以most voted的答案作为label(交叉熵loss)/从所有答案中依概率随机sample一个答案作为label(交叉熵loss)<br>分布预测：所有人工标注的正确答案用一个概率分布表示，计算真实分布和预测分布之间的KL散度<br>语言生成：to-do</p>
<h3 id="五、方法分类"><a href="#五、方法分类" class="headerlink" title="五、方法分类"></a>五、方法分类</h3><h4 id="（一）按照多模态特征融合的方法不同，可以分为："><a href="#（一）按照多模态特征融合的方法不同，可以分为：" class="headerlink" title="（一）按照多模态特征融合的方法不同，可以分为："></a>（一）按照多模态特征融合的方法不同，可以分为：</h4><ul>
<li>单线性（点加、点乘、连接）</li>
<li>双线性：矩阵外积</li>
</ul>
<p>最简单的双线性融合如下式：</p>
<p><img src="/2019/图像问答（VQA）/B5F2B787-F419-420A-A845-18876B847D03.png" alt="f19989356542fa0585b9ef8189f27a14"></p>
<p>其中x是m维图像特征，y是n维文本向量，Wi是(m,n)维的矩阵。<br>存在的问题：参数量过大，难以训练，而且容易过拟合；直接计算外积，计算量太大。<br>几种改进算法：</p>
<ul>
<li>MLB</li>
<li>MCB[6]</li>
<li>MUTAN</li>
<li>MFB[7]</li>
</ul>
<h4 id="（二）按照视觉特征的抽象层次不同"><a href="#（二）按照视觉特征的抽象层次不同" class="headerlink" title="（二）按照视觉特征的抽象层次不同"></a>（二）按照视觉特征的抽象层次不同</h4><ul>
<li>CNN中间层特征（低级视觉特征）[1,2,3,4,5]</li>
<li>CNN高层特征（高级语义特征）[1,3,13]</li>
</ul>
<p>下图是近年来比较有代表性的一些工作。<br>第一部分：无visual attention<br>第二部分：有visual attention，无semantic feature<br>第三部分：有semantic feature<br><img src="/2019/图像问答（VQA）/053096BE-ED9B-4E8E-83B3-14D19A679495.png" alt="52508c0663483e09bd5b35e760a30ddc"></p>
<h4 id="（三）按照注意力模型的不同"><a href="#（三）按照注意力模型的不同" class="headerlink" title="（三）按照注意力模型的不同"></a>（三）按照注意力模型的不同</h4><p>根据注意力模型中的查询对象和被查询对象不同，可以分为以下几种：</p>
<ul>
<li>问题 -&gt; 低级视觉特征（区域特征）（top-down attention）[1,2,3,4,5]</li>
<li>问题 -&gt; 高级视觉特征（语义特征）[1]</li>
<li>低级视觉特征（区域特征）-&gt; 问题[2]</li>
<li>低级视觉特征（区域特征）-&gt; 高级视觉特征（语义特征）[3]</li>
<li>图像本身 -&gt; 低级视觉特征（区域特征）（bottom-up attention）[4,5]</li>
<li>问题本身 -&gt; 问题特征[2]</li>
</ul>
<h5 id="1-Multi-level-attention-networks-for-visual-question-answering-MLAN-2017cvpr"><a href="#1-Multi-level-attention-networks-for-visual-question-answering-MLAN-2017cvpr" class="headerlink" title="[1]Multi-level attention networks for visual question answering(MLAN, 2017cvpr)"></a>[1]Multi-level attention networks for visual question answering(MLAN, 2017cvpr)</h5><ul>
<li>基于文本的语义注意力：因为并不是图像的所有concept都和问题有关，所以需要用attention模型来侧重那些关系密切的concepts。本文用问题特征作为query，来计算concepts的注意力权重，得到attended semantic feature；</li>
<li>基于上下文的低级视觉特征：与之前的工作独立提取各图像区域的特征不同，这篇论文考虑了不同区域之间的交互，它用一个BRNN来编码图像的上下文环境，然后用BRNN的隐藏状态作为context-aware visual feature，取代原来的各区域独立的visual feature。</li>
</ul>
<p>模型结构如下图所示：<br><img src="/2019/图像问答（VQA）/9D545378-993F-4FD6-A794-49D9616D2FC4.png" alt="7451023116a4c978834cbb96f72ce823"></p>
<p><strong>1.semantic attention：</strong><br>首先，一个CNN从图像中提取特征，然后通过一个sigmoid层，输出每个concept在该图像中出现的概率（concept是从数据集中提取的出现频率最高的一些词汇，每个concept都用一个单词表示，concept与问题中的单词共享同一个词汇表和嵌入矩阵）。这部分称为concept detector，是在数据集上预训练好的。接下来，所有concept对应的one-hot向量，都经过词嵌入，再经过一个线性映射，变成与问题特征相同的维度。最后，两者作点乘，再和前面求出的concepts的概率分布相乘，得到attention weights，作pooling得到最终的semantic feature。</p>
<p><img src="/2019/图像问答（VQA）/71EDED55-5510-417D-AA87-3B3336C8FDCE.png" alt="656c9fce78fc5b9af8765877beb5d512"><br><img src="/2019/图像问答（VQA）/C8291092-CB6B-4AA7-9EB2-35E083EED660.png" alt="74534ec629306ac98eb3a6c0742a1137"><br><img src="/2019/图像问答（VQA）/C558220F-B162-435B-8575-52996FD90949.png" alt="b50d71140e3b1af87764d91ba7400aa9"><br><img src="/2019/图像问答（VQA）/175F5195-6CA3-4DDD-A85B-7C9D55914B47.png" alt="519c90508787878ac69bd8c451265336"></p>
<p><strong>2.context-aware visual feature：</strong><br>将每个区域的特征排成一列，依次输入BRNN，然后取每个时刻BRNN的两个隐藏状态相加，得到相应区域的context-aware visual feature（效果并不好，可能是RNN只能有效地提取序列信息，而不能提取具有2维结构的数据特征）。接下来用问题特征作为查询，求出attended visual feature。<br><img src="/2019/图像问答（VQA）/540E9797-24C1-4DD6-905D-69F958A314D3.png" alt="7621bcf9e79433a8074526bc70176462"><br><img src="/2019/图像问答（VQA）/F178F8E9-89B1-4D4B-A63D-ADB3ABB69513.png" alt="bfc07b55958ab1f939f3d23de645edfc"><br><img src="/2019/图像问答（VQA）/0F7A3BEC-05AB-4D69-B30C-749BC34529BE.png" alt="4bf7b1264c48699f8c74754d384d8511"></p>
<p><strong>3.joint learning：</strong><br>visual feature和semantic feature分别与question feature相加，然后作点乘，最后经过softmax层输出概率分布。</p>
<p><img src="/2019/图像问答（VQA）/166CAA4C-1F72-48BB-B4B6-73DA97E07BF4.png" alt="8dcfa5b5306e4084f3727c9e060ed4a1"></p>
<h5 id="2-Hierarchical-Question-Image-Co-Attention-for-Visual-Question-Answering"><a href="#2-Hierarchical-Question-Image-Co-Attention-for-Visual-Question-Answering" class="headerlink" title="[2]Hierarchical Question-Image Co-Attention for Visual Question Answering"></a>[2]Hierarchical Question-Image Co-Attention for Visual Question Answering</h5><ul>
<li>多层次文本特征：提取了问题的单词/短语/句子三个层次的特征。</li>
<li>文本-图像互注意力：不仅用文本特征来计算图像各区域的权重，也用图像特征来计算文本中各单词/短语/句子的权重；另外还提出两种计算互注意力的方式，一种是同步（平行）计算，一种是异步（轮流）计算。</li>
</ul>
<p><strong>1.多层次文本特征：</strong><br>（1）单词：通过词嵌入获得每个单词的特征；<br><img src="/2019/图像问答（VQA）/701A61F8-EFB8-4873-9C7F-ADB1E2B1FBF6.png" alt="9f1d60d18994d0bf751bfce2356a5477"></p>
<p>（2）短语：通过对单词特征作1维卷积（使用3个卷积核，长度分别为1，2，3），取得问题的一/二/三元短语特征，作max pooling得到最终的短语特征；<br><img src="/2019/图像问答（VQA）/D9B5B5A7-C73F-4A09-890E-B127036FCCAD.png" alt="4374b6ce16a2f683e4132d985c661e43"><br><img src="/2019/图像问答（VQA）/58A92086-79FA-4A2C-848A-02406FBDD3EC.png" alt="a2918140ca061dc493ee7a163a94ec80"></p>
<p>（3）句子：通过用LSTM对短语特征进行编码，得到每时刻句子级别的文本特征。</p>
<p><strong>2.文本-图像互注意力：</strong><br>两种计算方式的计算图对比如下所示：<br><img src="/2019/图像问答（VQA）/9165884D-FFFD-44E7-889A-B0DD6CEBEB11.png" alt="3a5206a18cd10dde16b92a3cea5b2b62"><br>（1）平行计算：（公式的含义没有理解）</p>
<p><img src="/2019/图像问答（VQA）/D497B35E-7E57-4B64-8EFA-1965810AF8FB.png" alt="4098a014ebb9485484d69d185475294d"><br><img src="/2019/图像问答（VQA）/17826003-B858-4A6E-9282-21D5CB54FCEF.png" alt="60e1777390fe57d37b82dd83c1575d20"><br>（2）轮流计算：<br>假设待计算注意力的特征集合为X，用于计算注意力的查询特征是g。那么计算注意力的流程可以用一下公式统一概括：<br><img src="/2019/图像问答（VQA）/D3FDFF2B-996B-4505-B2C3-E2519EC7D417.png" alt="b7cb1022d59b088c20089023ceb8e3e5"><br>轮流计算文本和图像注意力的流程为：<br>X = Q, g = 0:相当于计算文本的自注意力，这一步获得attended question feature q；<br>X = V, g = q:基于文本的图像注意力，这一步获得attended visual feature v;<br>X = Q, g = v:基于图像的文本注意力，这一步获得final attended question feature q.</p>
<p>（3）推断：<br>由于文本特征有3个不同的level，这篇论文对每个level都分别计算了注意力。<br>推断的流程如以下公式所述：</p>
<p><img src="/2019/图像问答（VQA）/04ACEC95-EC4B-4DB2-8E75-B1CFEBB2FC47.png" alt="217bae2de67d4909b3c58e3d47643b23"></p>
<h5 id="3-R-VQA-Learning-Visual-Relation-Facts-with-Semantic-Attention-for-Visual-Question-Answering-RelAtt-2018cvpr"><a href="#3-R-VQA-Learning-Visual-Relation-Facts-with-Semantic-Attention-for-Visual-Question-Answering-RelAtt-2018cvpr" class="headerlink" title="[3]R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering(RelAtt, 2018cvpr)"></a>[3]R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering(RelAtt, 2018cvpr)</h5><ul>
<li>引入关系作为语义特征：不仅使用concepts和attributes作为图像的语义特征，还引入了主体之间的关系作为语义特征的补充。</li>
<li>用基于低级视觉特征的语义注意力替代基于文本的视觉注意力。</li>
</ul>
<p>模型结构如下图所示：</p>
<p><img src="/2019/图像问答（VQA）/7C9B24A7-0AE1-40D8-A64B-62CBB4C3F331.png" alt="3cba58d8c0c7721512eda17c3ec506d9"></p>
<p><strong>1.relation fact detector：</strong><br>用于检测图像中与问题相关的relation fact，这是这篇论文的关键点。<br>（1）数据集<br>每个relation fact可以用一个三元组（subject，relation，object）来表示。在Visual Genome数据集中，对每个图片，除了标注有图片中的物体、图片的属性之外，还标有图片中主体之间的关系（relation fact）。不过，数据集中没有标注哪些关系是和特定问题最相关的。本文通过计算每个关系与（图像-问题）对的相似性，给每个（图像-问题）对都标注了与其最相关的关系，形成了一个扩展的数据集R-VQA。<br>（2）模型<br>接下来，就可以用监督学习的方式来训练relation fact detector了。具体地说，首先从问题中提取文本特征，再从图像中提取视觉特征，将两种特征作融合，然后通过三个线性分类器，分别输出每个subject，relation，object与（图像-问题）对相关的概率（由于数据集中的subject，relation，object数目有限，所以可以把relation fact detect视为一个分类问题）。其模型如下图所示：<br><img src="/2019/图像问答（VQA）/542F4172-8A18-442A-B0AB-D2FF06273108.png" alt="f854f74362dd629c50dbbf744c16bfab"><br>（3）检测<br>假设每个relation fact出现的概率是它的三个元素出现概率之和，那么给定三个元素各自的概率分布，可以求出前k个最相关的relation fact，每个都由3个one-hot向量构成，首先把它们嵌入到共同特征空间中，然后备用。</p>
<p><strong>2.semantic attention：</strong><br>首先用GRU提取问题的文本特征，然后用ResNet提取图像的区域特征，两者作MLB融合，经过softmax层输出attention map，得到attended visual feature。再用它和1中得到的relation facts作MLB融合，经过softmax得到attention weights，最后计算attended relation fact，将其作为最终的semantic feature。</p>
<h5 id="4-Tips-and-Tricks-for-Visual-Question-Answering"><a href="#4-Tips-and-Tricks-for-Visual-Question-Answering" class="headerlink" title="[4]Tips and Tricks for Visual Question Answering"></a>[4]Tips and Tricks for Visual Question Answering</h5><h5 id="5-Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering"><a href="#5-Bottom-Up-and-Top-Down-Attention-for-Image-Captioning-and-Visual-Question-Answering" class="headerlink" title="[5]Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"></a>[5]Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</h5><h5 id="6-MCB-Multimodal-Compact-Bilinear-Pooling"><a href="#6-MCB-Multimodal-Compact-Bilinear-Pooling" class="headerlink" title="[6]MCB:Multimodal Compact Bilinear Pooling"></a>[6]MCB:Multimodal Compact Bilinear Pooling</h5><p><img src="/2019/图像问答（VQA）/9A0CE23A-B464-4508-9F22-486963C86551.png" alt="6f78ba46ca894f1a71bdc3700e159337"><br>（1）count sketch算法：总之是一种把n维向量映射为d维向量的算法。它的性质是：两个向量外积的count sketch，等于两个向量count sketch的卷积。<br><img src="/2019/图像问答（VQA）/92873096-8D89-4E66-95F4-9719472DA466.png" alt="2347fcf822a2d0a736240e48e836df98"></p>
<p>（2）MCB流程：v（2048）和q（2048）都经过count sketch转换为16k维向量v’和q’。然后v’和q’分别作傅里叶变换，点乘后作逆变换，得到v和q外积的count sketch。<br><img src="/2019/图像问答（VQA）/712F45A2-B6E7-4914-91EE-91ECBBA4278B.png" alt="f8fa5249b8d93c96b87d35c9f591871e"></p>
<h5 id="7-MFB-Multimodal-Factorized-Bilinear-Pooling"><a href="#7-MFB-Multimodal-Factorized-Bilinear-Pooling" class="headerlink" title="[7]MFB:Multimodal Factorized Bilinear Pooling"></a>[7]MFB:Multimodal Factorized Bilinear Pooling</h5><p>MFB借助了矩阵分解的思想来解决双线性运算参数量大的问题。矩阵W可以视为两个低阶矩阵U和V的乘积：</p>
<script type="math/tex; mode=display">W_{m,n} = U_{m,k} * V_{k,n}</script><script type="math/tex; mode=display">z_{i} = x^{T}U_{i}V_{i}^{T}y = (x^{T}U_{i})(y^{T}V_{i})</script><p>其中k远小于m和n。计算图如下所示：</p>
<p><img src="/2019/图像问答（VQA）/E49158F7-8A78-4AC4-9A3E-EE18349CEC56.png" alt="ae743530675a6dd0d0c8658979686c13"></p>
<h5 id="8-Generating-Visual-Explanations"><a href="#8-Generating-Visual-Explanations" class="headerlink" title="[8]Generating Visual Explanations"></a>[8]Generating Visual Explanations</h5><p>这篇论文研究可解释图像分类问题。具体地说，它要生成一个自然语言句子，用来解释图像分类模型将一张图片分为特定类别的原因。</p>
<p>没太看懂。有几个明显的问题：<br>1.相关度损失与辨识度损失存在一定的冲突：<br>如果模型生成的对一张图片分类结果的解释有很高的辨识度，那么它很可能和这张图片的描述很不一致，因为它可能略去了一些在描述中出现而和分类结果无关的部分，同时又加入了一些非常细微的、在描述中没有出现的部分。<br>2.辨识度损失的设计不够合理：<br>作者用图片的描述和类别标记作为训练数据，训练一个LSTM分类器，输出一张图片的描述，输出图片的类别。如果把它输出的概率作为奖励函数，那么存在的问题是：生<br>成的解释越接近图片的描述越好，但设计辨识度损失的本来目的，是要让生成的解释具有类别辨识度，而不要和图片的描述完全一致，这是自相矛盾的。</p>
<ul>
<li>将图像特征与类别信息融合起来，通过LSTM生成自然语言解释</li>
<li>用相关度损失和辨识度损失来训练模型，使生成的解释1.与图像相关（一致性）；2.有辨识度（能够解释做出指定分类决策的理由）</li>
</ul>
<p>模型结构如下图所示：<br><img src="/2019/图像问答（VQA）/8472049E-8802-4097-906E-5622F355335F.png" alt="73561bfe27c6fa0d456542d5f22ec073"></p>
<p>模型的两个损失函数：<br>1.相关度损失：<br><img src="/2019/图像问答（VQA）/AD17B823-05BD-4985-923A-CC4FC7A67F14.png" alt="db1706a0157296f8029a1a2c0478b0f8"><br>2.辨识度损失：<br><img src="/2019/图像问答（VQA）/D89D0F89-8CE7-4331-9996-D6186D41E92E.png" alt="0e40588635066600b237f40d451b24f6"><br><img src="/2019/图像问答（VQA）/2EB8DC9D-9F5E-4B28-9732-E3BA04A73E3F.png" alt="cbb6333fb8f65530778b8950a0bbd67e"></p>
<p>模型的训练过程如下图所示:<br><img src="/2019/图像问答（VQA）/8F2379C2-4313-4929-8306-718546E90375.png" alt="a04e688b90b60a2c347f26983819c2a1"></p>
<h5 id="9-Generating-Natural-Language-Explanations-for-Visual-Question-Answering-Using-Scene-Graphs-and-Visual-Attention"><a href="#9-Generating-Natural-Language-Explanations-for-Visual-Question-Answering-Using-Scene-Graphs-and-Visual-Attention" class="headerlink" title="[9]Generating Natural Language Explanations for Visual Question Answering Using Scene Graphs and Visual Attention"></a>[9]Generating Natural Language Explanations for Visual Question Answering Using Scene Graphs and Visual Attention</h5><p>这篇论文同样可解释VQA的问题。与以往的研究直接使用人工标注的解释来进行训练相比，这篇论文不需要完整的人工解释，只需要一些人工标注的片段信息，比如图片中各区域的描述，以及实体与实体间的关系。这些在现有的VQA数据集中是可获取的。</p>
<p>首先采用最简单的VQA模型来预测答案（ResNet提取低级视觉特征，LSTM提取文本特征，使用基于文本的视觉注意力机制，生成一张attention map，将pooling之后的视觉特征与文本特征作连接，经过一个线性分类器输出答案）。接下来，根据问题Q、答案A以及attention map，生成答案的解释。</p>
<p>模型如下图所示：<br><img src="/2019/图像问答（VQA）/DE0293F2-FEB1-4B31-B7F1-DF276527EF61.png" alt="641a497840644816d6d2c791d86c7942"></p>
<p>1.Entity/Relation/Region extractor：<br>从图片中提取实体、关系、区域描述等信息，用作生成解释的原料。这些信息可能是数据集中自带的，也可能需要用一个模型来提取。</p>
<p>2.Model to generate attention map：<br>即注意力模型，它根据文本特征，生成一张attention map。</p>
<p>3.Candidate phrase generator and language model scorer:<br>为了选择与问题和答案相关的区域描述/实体/关系，需要衡量这些信息与问题和答案之间的相关度：<br><img src="/2019/图像问答（VQA）/9A086781-5796-4D58-AB81-505C402A80A5.png" alt="40fba04d1bb6bc46d70915ea2260b76a"></p>
<p>其中D表示一个区域的描述，R(D)是它所对应的区域，相关度与两个因素有关：attentionScore - R(D)与attention map中的热点区域交集越大，表明它越可能与答案相关（为了抵消区域之间大小不一致造成的不公平，后面又除了区域面积的对数）；lmScore - 衡量在一个预训练的语言模型中，给定上文Q和A，下文出现D的概率（为了抵消对长句的不公平，后面又乘了句子长度的平方根）。</p>
<p>4.Explanation generator:<br>计算完相关度之后，接下来就生成最终的解释。分两种情况讨论：<br>（1）有区域描述：这时就按照相关度，输出最相关的几条描述作为解释。<br>（2）有实体/关系：这时首先按照实体与它们之间的关系，建立一张scene graph，以实体作为图的顶点，以实体间的关系作为图的边。接下来，用深度优先搜索的方法（这里没看懂是什么操作），获得最终的解释。</p>
<h5 id="10-Multimodal-Explanations-Justifying-Decisions-and-Pointing-to-the-Evidence"><a href="#10-Multimodal-Explanations-Justifying-Decisions-and-Pointing-to-the-Evidence" class="headerlink" title="[10]Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"></a>[10]Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</h5><p>这篇论文同样研究可解释VQA问题，主要贡献如下:</p>
<ul>
<li>视觉解释：根据答案，生成一张图像的attention map，用来指示图像中支持此答案的区域；</li>
<li>文字解释：将问题、图像、答案三方面的特征融合后，用来生成解释</li>
<li>监督学习：作者收集了一个VQA-X数据集，其中包含人工标注的每个图片-问题对的答案、视觉解释和文字解释，利用这个数据集，通过监督学习训练模型</li>
</ul>
<p>模型结构如下图所示：<br><img src="/2019/图像问答（VQA）/1FD69FA8-607C-4266-A7A5-2C3609DE2560.png" alt="4bd1f10019e299a50474a6080d40499d"></p>
<h5 id="11-VQA-E-Explaining-Elaborating-and-Enhancing-Your-Answers-for-Visual-Questions"><a href="#11-VQA-E-Explaining-Elaborating-and-Enhancing-Your-Answers-for-Visual-Questions" class="headerlink" title="[11]VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"></a>[11]VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions</h5><p>这篇论文同样研究可解释VQA问题，主要贡献如下:</p>
<ul>
<li>采用迁移学习的方法，同时训练答案预测模型和解释生成模型，使两者的表现都有所提升</li>
<li>作者用自动化的方法收集了一个VQA-E数据集</li>
</ul>
<p>1.迁移学习：<br>模型结构如下所示：<br><img src="/2019/图像问答（VQA）/7CF950FC-AF86-440E-A5A8-BDBDB984C44F.png" alt="554c888b444b13c6fab807475faa6b6c"></p>
<p>2.数据集VQA-E：<br>为了VQA数据集中的每个IQA triplelet生成相应的ground truth explanation，作者借助于图像的描述。具体地，对每个IQA triplelet，作者首先找到与图像I最相近的描述C，相似度的计算如下所示：<br><img src="/2019/图像问答（VQA）/2F86B188-092B-4AA2-A9DF-0234F43C6D6F.png" alt="44714575cfba63d283c70adf5ced21af"><br>其中的w表示词嵌入向量。<br>接下来，对每个IQAC四元组，作者首先将Q和A合并为一个陈述性语句S，然后获取S和C的语法树，找到其中相同的节点，最后用S中以该节点的父节点为根的子树替换C中以该节点的父节点为根的子树，就得到解释E的语法树。<br><img src="/2019/图像问答（VQA）/5C103734-A563-4C55-AD96-8E82FF3A8FAE.png" alt="63bcf64b642304cf8d100131483768e3"></p>
<h5 id="12-Using-Explanations-to-Improve-Ensembling-of-Visual-Question-Answering-Systems"><a href="#12-Using-Explanations-to-Improve-Ensembling-of-Visual-Question-Answering-Systems" class="headerlink" title="[12]Using Explanations to Improve Ensembling of Visual Question Answering Systems"></a>[12]Using Explanations to Improve Ensembling of Visual Question Answering Systems</h5><p>（待看）主要是将现有的几个比较好的VQA模型作融合。</p>
<h5 id="13-Tell-and-Answer-Towards-Explainable-Visual-Question-Answering-using-Attributes-and-Captions"><a href="#13-Tell-and-Answer-Towards-Explainable-Visual-Question-Answering-using-Attributes-and-Captions" class="headerlink" title="[13]Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions"></a>[13]Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions</h5><ul>
<li>同样属于利用语义特征的line，但它提取的语义特征，除实体和属性之外，还有图像的描述（如此能够把握实体之间的关系）</li>
</ul>
<p>模型结构如下所示：<br><img src="/2019/图像问答（VQA）/D1D645F0-8774-4ACE-A319-6FA5E495DB32.png" alt="228687f863012f685e61333983a8bd13"></p>
<p>1.word-level semantic feature：多标签分类<br>2.sentence-level semantic feature：预训练的caption模型</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://MMLabSzF205.github.io/2019/NLP中的迁移学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MMLabSzF205">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="F205学习小组博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/NLP中的迁移学习/" itemprop="url">
                  NLP中的迁移学习
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 23:10:53 / Modified: 09:31:15" itemprop="dateCreated datePublished" datetime="2019-06-11T23:10:53+08:00">2019-06-11</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h1><h2 id="1-1-什么是迁移学习"><a href="#1-1-什么是迁移学习" class="headerlink" title="1.1. 什么是迁移学习"></a>1.1. 什么是迁移学习</h2><ul>
<li>假如一个人从来没有见过猫，那么这个人只需要最多10张照片，就能学习到什么是猫。</li>
<li>当你拿到一张照片的时候，在认识猫之前你已经认识了太多的东西了。<br><img src="http://wx2.sinaimg.cn/large/007mOCDngy1g3wa9yjg5fj30ns0hedwy.jpg" alt="image"><br><img src="http://wx1.sinaimg.cn/large/007mOCDngy1g3v0v1d2nmj30w60mednx.jpg" alt="image"></li>
</ul>
<h2 id="1-2-为什么需要迁移学习"><a href="#1-2-为什么需要迁移学习" class="headerlink" title="1.2. 为什么需要迁移学习"></a>1.2. 为什么需要迁移学习</h2><ul>
<li>许多任务共享相同的语言知识</li>
<li>有标注的数据较少</li>
<li>经验上，引入迁移学习取得巨大成功</li>
</ul>
<p><img src="http://ws2.sinaimg.cn/large/007mOCDngy1g3v13llgpnj31w80ucwo3.jpg" alt="image"></p>
<h2 id="1-3-迁移学习分类"><a href="#1-3-迁移学习分类" class="headerlink" title="1.3. 迁移学习分类"></a>1.3. 迁移学习分类</h2><p>把我们当前要处理的NLP任务叫做T（T称为目标任务），迁移学习技术做的事是利用另一个任务S（S称为源任务）来提升任务T的效果，也即把S的信息迁移到T中。</p>
<ul>
<li>S无监督，且源数据和目标数据同时用于训练：此时主要就是自监督（self-supervised）学习技术，代表工作有CVT。</li>
<li><strong>S无监督，且先使用源数据训练，再使用目标数据训练（序贯训练）：此时主要就是以BERT为代表的无监督模型预训练技术，代表工作有ELMo、ULMFiT、GPT/GPT-2、BERT、MASS、UNILM。（NLP中的迁移学习大多数属于该类）</strong></li>
<li>S有监督，且源数据和目标数据同时用于训练：此时主要就是多任务（multi-task）学习技术，代表工作有MT-DNN。</li>
<li>S有监督，且先使用源数据训练，再使用目标数据训练（序贯训练）：此时主要就是有监督模型预训练技术，类似CV中在ImageNet上有监督训练模型，然后把此模型迁移到其他任务上去的范式。代表工作有CoVe。</li>
</ul>
<h2 id="1-4-序贯训练"><a href="#1-4-序贯训练" class="headerlink" title="1.4. 序贯训练"></a>1.4. 序贯训练</h2><p><img src="http://ws3.sinaimg.cn/large/007mOCDngy1g3v1figi17j31pa0py421.jpg" alt="image"></p>
<h3 id="1-4-1-无监督预训练"><a href="#1-4-1-无监督预训练" class="headerlink" title="1.4.1. 无监督预训练"></a>1.4.1. 无监督预训练</h3><ul>
<li>易于得到大量无标注语料：维基百科，新闻，社交媒体…</li>
<li>使用语言模型且基于假设：你可以通过某个词的上下文得知该词的意思</li>
</ul>
<h3 id="1-4-2-有监督预训练"><a href="#1-4-2-有监督预训练" class="headerlink" title="1.4.2. 有监督预训练"></a>1.4.2. 有监督预训练</h3><ul>
<li>CV中常见，NLP中缺乏大量标注数据集故不常用</li>
<li>机器翻译</li>
<li>S与T相关程度高（eg.从一个Q&amp;A数据集训练出的模型迁移到另一个数据集上）</li>
</ul>
<h3 id="1-4-3-目标任务"><a href="#1-4-3-目标任务" class="headerlink" title="1.4.3. 目标任务"></a>1.4.3. 目标任务</h3><ul>
<li>情感分析，问答，文本分类…</li>
</ul>
<h1 id="2-预训练"><a href="#2-预训练" class="headerlink" title="2. 预训练"></a>2. 预训练</h1><p><img src="http://ws4.sinaimg.cn/large/007mOCDngy1g3v1tsrjpcj31v00w4dt9.jpg" alt="image"></p>
<h2 id="2-1-为什么语言模型可以work"><a href="#2-1-为什么语言模型可以work" class="headerlink" title="2.1. 为什么语言模型可以work"></a>2.1. 为什么语言模型可以work</h2><ul>
<li>即使是对于人来说，这样是比较困难的任务</li>
<li>语言模型会将可能词汇压缩进一个向量（e.g.“They walked down the street to ???”）</li>
<li>为了实现该任务，模型将被迫学习语义，情感等信息</li>
</ul>
<h2 id="2-2-生成向量学到了什么"><a href="#2-2-生成向量学到了什么" class="headerlink" title="2.2. 生成向量学到了什么"></a>2.2. 生成向量学到了什么</h2><p><img src="http://ws3.sinaimg.cn/large/007mOCDngy1g3vuw0x2c7j30j80goq4w.jpg" alt="image"><br><img src="http://wx4.sinaimg.cn/large/007mOCDngy1g3vv3r7kzbj30t80ikq61.jpg" alt="image"><br><strong>BERT:fine tuning&gt;最后四层连接&gt;最后四层相加&gt;倒数第二层&gt;倒数第一层</strong></p>
<h1 id="3-迁移"><a href="#3-迁移" class="headerlink" title="3. 迁移"></a>3. 迁移</h1><ul>
<li>模型结构修改：预训练的模型应该修改到怎样的程度来做下游任务？</li>
<li>优化策略：哪些层的参数需要训练，又该按怎样的顺序？</li>
</ul>
<h2 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1. 模型"></a>3.1. 模型</h2><ul>
<li>不调整预训练模型中间层</li>
<li>修改模型中间层</li>
</ul>
<h3 id="3-1-1-unchanged"><a href="#3-1-1-unchanged" class="headerlink" title="3.1.1. unchanged"></a>3.1.1. unchanged</h3><ol>
<li><p>移除预训练模型最后一层<br><img src="http://ws4.sinaimg.cn/large/007mOCDngy1g3vwih3cjzj30k20i0afu.jpg" alt="image"></p>
</li>
<li><p>根据特定任务添加相关层<br><img src="http://wx2.sinaimg.cn/large/007mOCDngy1g3vwmzntf4j30na0q0wmx.jpg" alt="image"></p>
</li>
</ol>
<h3 id="3-1-2-changed"><a href="#3-1-2-changed" class="headerlink" title="3.1.2. changed"></a>3.1.2. changed</h3><ul>
<li>原因：目标任务与预训练任务要求结构差距较大（S:单句输入，T：多句输入）</li>
<li>Use the pretrained model weights to initialize as much as possible of a structurally different target task model<br><img src="http://ws4.sinaimg.cn/large/007mOCDngy1g3wapk82mlj30t40lqdrr.jpg" alt="image"></li>
</ul>
<h2 id="3-2-优化"><a href="#3-2-优化" class="headerlink" title="3.2. 优化"></a>3.2. 优化</h2><p>可选操作</p>
<ul>
<li>哪些层的权重需要更新(feature based ,fine tuning)</li>
<li>什么时候更新，怎样更新(from top to bottom,gradual ufreezing)</li>
</ul>
<h3 id="3-2-1-which-weights"><a href="#3-2-1-which-weights" class="headerlink" title="3.2.1. which weights"></a>3.2.1. which weights</h3><p><strong>to tune or not to tune(the pretrained weights)</strong><br>1.不微调<br><img src="http://wx4.sinaimg.cn/large/007mOCDngy1g3w4rmaw3ij30ne0k0tg9.jpg" alt="image"><br><img src="http://wx3.sinaimg.cn/large/007mOCDngy1g3w4tcfdqnj30sw0kunbt.jpg" alt="image"></p>
<p>2.微调</p>
<h3 id="3-2-3-更新策略"><a href="#3-2-3-更新策略" class="headerlink" title="3.2.3. 更新策略"></a>3.2.3. 更新策略</h3><p>动机：避免重写有用信息，最大化正向迁移<br>相关概念：catastrophic forgetting（resNet）</p>
<ul>
<li>整个训练阶段冻结除最后层之外所有层</li>
<li>自底向上每次训练一层</li>
<li>自上向下逐步解冻</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://MMLabSzF205.github.io/2019/Test/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MMLabSzF205">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="F205学习小组博客">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/Test/" itemprop="url">
                  Test
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-10 23:10:53" itemprop="dateCreated datePublished" datetime="2019-06-10T23:10:53+08:00">2019-06-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-06-11 09:15:16" itemprop="dateModified" datetime="2019-06-11T09:15:16+08:00">2019-06-11</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="MMLabSzF205">
            
              <p class="site-author-name" itemprop="name">MMLabSzF205</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MMLabSzF205</span>

  

  
</div>


  



  <div class="powered-by">Powered by <a class="theme-link" target="_blank" rel="external nofollow" href="https://hexo.io">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" rel="external nofollow" href="https://theme-next.org">NexT.Pisces</a> v6.4.2</div>




        








        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.2"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  



  










  





  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
  

  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('Copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
